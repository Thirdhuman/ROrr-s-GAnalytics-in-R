df1[j,i]=	ifelse(grepl(df1[j,10],colnames(df1[i]))==colnames(df1[i]), TRUE, FALSE)}}
for (i in 66:length(df1)){
for (j in 79000:nrow(df1)) {
df1[j,i]=	ifelse(grepl(df1[j,10],colnames(df1[i]))==colnames(df1[i]), TRUE, FALSE)}}
View(df1)
df1$author=NULL
df2 = data.frame()
get_data=function(vid,from,to,dim,met,max){df=google_analytics(
viewId=vid,date_range=c(from,to),metrics=met,dimensions=dim, #met_filters = fc,
dim_filters = fc2,  max = max	,anti_sample = TRUE)
# clean up and set class
df$dimension1 = gsub('O&#039;Toole', "O'Toole", df$dimension1)
df$author_full=df$dimension1
df$dimension1 <- NULL
# df$author=name
# df$co_authors = gsub(name, '', df$author_full)
# df$co_authors = gsub("^,*|(?<=,),|,*$", "", df$co_authors, perl=T)
# df$co_authors=gsub(', , ', ', ', df$co_authors)
# df$co_authors=trimws(df$co_authors)
# df$collaboration_yn=ifelse(df$author==df$author_full,"Sole Author",
# 																				ifelse(df$author!=df$author_full|!is.na(df$co_authors),"Co-Authored",0))
df$ID <- paste0(sapply(lst, function(x) paste(x, collapse = '')), df$Year)
df}
gadata=get_data(vid=vid, from=from, to=to, dim=dim, met=met, max=max)
save(gadata, file = "Last_Raw_GA_DAT.RData")
#######
load( file = "Last_Raw_GA_DAT.RData")
#######
df1 = as.data.frame(gadata)
df1[as.character(authur_row)] <- NA_character_
# for( i in length(df1)){
# 		grepl("d", names(df1))
#
# }
#length((authur_row))
colnames(df1)
faster_loop=df1[ ,10:63]
colnames(faster_loop)
for (i in 3:length(faster_loop)){
for (j in 1:nrow(faster_loop)) {
faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i]))==colnames(faster_loop[i]), TRUE, FALSE)}}
View(faster_loop)
df1$ID=NULL
colnames(df1)
df1 = as.data.frame(gadata)
df1[as.character(authur_row)] <- NA_character_
df1$ID=NULL
colnames(df1)
faster_loop=df1[ ,10:62]
colnames(faster_loop)
for (i in 2:length(faster_loop)){
for (j in 1:nrow(faster_loop)) {
faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i]))==colnames(faster_loop[i]), TRUE, FALSE)}}
View(df1)
View(faster_loop)
ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i]))==colnames(faster_loop[i]))
ifelse(grepl(faster_loop[5,1],colnames(faster_loop[10]))==colnames(faster_loop[10]))
(grepl(faster_loop[5,1],colnames(faster_loop[10]))==colnames(faster_loop[10]))
faster_loop
head(faster_loop)
head(faster_loop, 1)
colnames(faster_loop[10])
grepl(faster_loop[5,1],colnames(faster_loop[10])
)
grepl(faster_loop[5,1],colnames(faster_loop[10]))
grepl(faster_loop[5,1],colnames(faster_loop[10]))
colnames(faster_loop)
grepl(faster_loop[1,1],colnames(faster_loop[32]))
for (i in 2:length(faster_loop)){
for (j in 1:nrow(faster_loop)) {
faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i])), TRUE, FALSE)}}
dput(faster_loop)
dput(faster_loop[1:15,1:6])
faster_loop=df1[ ,10:62]
dput(faster_loop[1:15,1:6])
df1 = as.data.frame(gadata)
df1[as.character(authur_row)] <- NA_character_
df1$ID=NULL
colnames(df1)
grepl(faster_loop[1,1],colnames(faster_loop[32]))
head(faster_loop, 1)
faster_loop=df1[ ,10:62]
colnames(faster_loop)
#dput(faster_loop[1:15,1:6])
for (i in 2:length(faster_loop)){
for (j in 1:nrow(faster_loop)) {
faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i])), TRUE, FALSE)}}
faster_loop[-1] <- lapply(names(faster_loop[-1]), function(nm) grepl(nm, faster_loop[[1]]))
View(faster_loop)
df1 = as.data.frame(gadata)
df1[as.character(authur_row)] <- NA_character_
df1$ID=NULL
colnames(df1)
grepl(faster_loop[1,1],colnames(faster_loop[32]))
head(faster_loop, 1)
faster_loop=df1[ ,10:62]
colnames(faster_loop)
faster_loop=df1[ ,1:9]
#dput(faster_loop[1:15,1:6])
for (i in 2:length(faster_loop)){
for (j in 1:nrow(faster_loop)) {
faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i])), TRUE, FALSE)}}
df1 = as.data.frame(gadata)
df1[as.character(authur_row)] <- NA_character_
df1$ID=NULL
colnames(df1)
grepl(faster_loop[1,1],colnames(faster_loop[32]))
head(faster_loop, 1)
faster_loop=df1[ ,10:62]
colnames(faster_loop)
faster_loop=df1[ ,1:9]
#dput(faster_loop[1:15,1:6])
# for (i in 2:length(faster_loop)){
# 	for (j in 1:nrow(faster_loop)) {
# faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i])), TRUE, FALSE)}}
#faster_loop[-1] <- lapply(names(faster_loop[-1]), function(nm) grepl(nm, faster_loop[[1]]))
df1[-9] <- lapply(names(df1[-9]), function(nm) grepl(nm, df1[[10]]))
View(df1)
colnames(df1)
df1 = as.data.frame(gadata)
df1[as.character(authur_row)] <- NA_character_
df1$ID=NULL
colnames(df1)
grepl(faster_loop[1,1],colnames(faster_loop[32]))
head(faster_loop, 1)
colnames(faster_loop)
df1_1=df1[ ,1:9]
df1_2=df1[ ,10:62]
#dput(faster_loop[1:15,1:6])
# for (i in 2:length(faster_loop)){
# 	for (j in 1:nrow(faster_loop)) {
# faster_loop[j,i]=	ifelse(grepl(faster_loop[j,1],colnames(faster_loop[i])), TRUE, FALSE)}}
df1_2[-1] <- lapply(names(df1_2[-1]), function(nm) grepl(nm, df1_2[[1]]))
#df1[-9] <- lapply(names(df1[-9]), function(nm) grepl(nm, df1[[10]]))
test=cbind(df1_1,df1_2)
test
View(test)
df1=cbind(df1_1,df1_2)
rm(gadata)
#####################################################
#### Initialize Cleanup of Google Analytics Data ####
#####################################################
df1$obs_day=as.Date(df1$date)
df1$date<-NULL
#  Remove Popular Proxy Strings
df1 <- df1[!grepl("search/srpcache", df1$pagePath),]
df1 <- df1[!grepl("www.filterbypass.me", df1$pagePath),]
df1 <- df1[!grepl("wikipedia.org/secure", df1$pagePath),]
df1 <- df1[!grepl("www.googleadservices.com", df1$pagePath),]
df1 <- df1[!grepl("bit.ly", df1$pagePath),]
df1 <- df1[!grepl("j.mp", df1$pagePath),]
df1 <- df1[!grepl("nl.hideproxy.me", df1$pagePath),]
df1 <- df1[!grepl("cc.bingj.com", df1$pagePath),]
df1 <- df1[!grepl("prolegis/getfile", df1$pagePath),]
df1 <- df1[!grepl("cluster23-files.instructure", df1$pagePath),]
df1 <- df1[!grepl("rorr.im/reddit.com", df1$pagePath),]
df1 <- df1[!grepl("www.duplichecker.com", df1$pagePath),]
df1 <- df1[!grepl("copyscape", df1$pagePath),]
df1 <- df1[!grepl("us1.proxysite", df1$pagePath),]
df1 <- df1[!grepl("eveil.alize", df1$pagePath),]
df1 <- df1[!grepl("honyaku.yahoofs.jp", df1$pagePath),]
df1 <- df1[!grepl("ow.ly", df1$pagePath),]
df1 <- df1[!grepl("searchenginereports.net", df1$pagePath),]
df1 <- df1[!grepl("xitenow", df1$pagePath),]
df1 <- df1[!grepl("cato.us1.list-manage.com/track/click", df1$pagePath),]
# More Reshaping of Page Paths & Remove Trailing Strings #
df1$pagePath2= gsub(".*www.cato.org", "www.cato.org", df1$pagePath)
df1$pagePath2= gsub(pattern="[?].*","",x=df1$pagePath2)
df1$pagePath2= gsub(pattern=".*https://*|&.*","",x=df1$pagePath2)
df1$pagePath2= gsub(pattern=".*genius.it/*|&.*",replacement="",x=df1$pagePath2)
df1$pagePath2= gsub(".*www-cato-org","www.cato.org",df1$pagePath2,perl=TRUE)
df1$pagePath2= gsub(".*www.cato.org.*?/publications","www.cato.org/publications",df1$pagePath2,perl=TRUE)
df1$pagePath2= gsub("\\.myaccess.library.utoronto.ca", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("proxy.earlham.edu", "www.cato.org", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("object.cato.org", "www.cato.org", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("seekingalpha.com", "www.cato.org", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".ezproxy.wallawalla.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".stfi.re", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".helin.uri.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".proxy.lib.pdx.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("www.cato.org:80", "www.cato.org", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("www.cato-at-liberty.org", "www.cato.org/blog", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("www.cato/", "www.cato.org/", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("index.html", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".jpllnet.sfsu.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2=gsub("what-have-the-politicians-in-washington-given-us/","what-have-politicians-washington-given-us",df1$pagePath2)
df1$pagePath2= gsub(".ezproxy.csusm.edu", "", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("proxy.unfake.us/proxy/350/", "www.cato.org/blog/", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2=ifelse(grepl("php$", df1$pagePath2)==T, df1$pagePath, df1$pagePath2)
refcols <- c("obs_day", 'pagePath', 'pagePath2')
df1 <- df1[, c(refcols, setdiff(names(df1), refcols))]
df1$pagePath1=df1$pagePath
df1$pagePath=df1$pagePath2
df1$pagePath2=NULL
#####################################################
################ Scrape Cato Web Data ###############
#####################################################
df_final= data.frame()
web_df= data.frame()
df1=df1[grep(name, rownames(df1)), ]
df1$ID <- seq.int(nrow(df1))
url_vector_full=df1[["pagePath"]]
url_vector=unique(url_vector_full)
SafeGet = function (x)	{
tryCatch({
#	short_url_vector
html=GET(x)
parsed=htmlParse(html)
root=xmlRoot(parsed)
title = xpathSApply(root, "//h1[@class='page-h1'][1]", xmlValue)
return(title)},
error=function(e){cat("ERROR :", conditionMessage(e))}, '0')}
website_responses <- pbmclapply(url_vector, SafeGet, mc.preschedule=T)
title=trimws(website_responses)
link_df=as.data.frame(cbind(title=title, pagePath=url_vector))
link_df_full=as.data.frame(cbind(pagePath=url_vector_full))
linked_title= merge(link_df_full, link_df, by=('pagePath'), all.x=T)
save(title, file = "Title_Vector.RData")
#######
load( file = "Title_Vector.RData")
linked_title=unique(linked_title)
df_intermediate = merge(df1, linked_title, by.x = 'pagePath', by.y = 'pagePath', all.x=T)
type_list <- pbmclapply(url_vector, function(url){
type = gsub('www.cato.org*/|/.*', "\\1", url)
type = gsub('-', " ", type)
type_2 = gsub('www.cato.org/publications*/|/.*', "\\1", url)
type_2 = gsub('-', " ", type_2)
type=ifelse((type=="publications"), type_2, type)
})
type_df=data.frame(cbind(type=type_list))
type_df=as.data.frame(cbind(type=type_list, pagePath=url_vector))
type_df_full=as.data.frame(cbind(pagePath=url_vector_full))
linked_type= merge(type_df_full, type_df, by=('pagePath'), all.x=T)
linked_type=unique(linked_type)
df_intermediate <- merge(df_intermediate, linked_type, by.x = 'pagePath', by.y = 'pagePath', all.x=T)
# Extract web content from Cato Website
df3= data.frame()
text_content <- df_intermediate %>%
distinct(pagePath, title, type)
text_content=text_content[!duplicated(text_content),]
text_url_vector=text_content[["pagePath"]]
text_responses <- pbmclapply(text_url_vector, GET)
body_vector = pbmclapply(text_responses, function (filename) {
doc = htmlParse(filename)
body = xpathSApply(doc, "//div[@class='field-body'][1]", xmlValue)
body =  gsub('\nNotes\n.*', '', body)
body =  gsub("\n", ' ', body)
body=trimws(body)})
body_count=pbmclapply(gregexpr("[[:alpha:]]+", body_vector), function(x) sum(x > 0))
pub_date_output = pbmclapply(text_responses, function (filename) {
doc = htmlParse(filename)
pub_date = xpathSApply(doc, "//meta[@name='publication_date'][1]",xmlGetAttr,'content')})
tags_output = pbmclapply(text_responses, function (filename) {
doc = htmlParse(filename)
tags = xpathSApply(doc, "//div[@class='field-tags inline']", xmlValue)
tags =  gsub("\n", ' ', tags)
tags=trimws((tags))})
topics_output = pbmclapply(text_responses, function (filename) {
doc = htmlParse(filename)
topics = xpathSApply(doc, "//div[@class='field-topics inline']", xmlValue)
topics =  gsub("\n", ' ', topics)
topics=trimws((topics))})
text_df=data.frame(cbind(body=body_vector,body_count=body_count,topics=topics_output,tags=tags_output,pub_date=pub_date_output ))
text_stats <- cbind(text_content, text_df)
# Text Analysis	- Generate Text Wall
text_wall <- text_df %>%
distinct(title, body,tags)
text_wall=text_wall[!duplicated(text_wall),]
for(i in 1:nrow(text_content)){
if (i==1){save_docs=paste(text_wall$title[i],text_wall$body[i],as.character(text_wall$tags[i]))}
else{save_docs = paste(save_docs,text_wall$title[i],text_wall$body[i],as.character(text_wall$tags[i]))}
}
library(tm)
library(wordcloud)
library(topicmodels)
library(quanteda)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
text_stats$row_count = NULL
## Text Analysis - Top Terms ##
for(k in 1:nrow(text_stats)){
keywords_doc = paste(text_stats$title[k],text_stats$body[k],text_stats$tags[k])
keywords_doc=Corpus(VectorSource(keywords_doc))
keywords_doc <- tm_map(keywords_doc, toSpace, "/")
keywords_doc <- tm_map(keywords_doc, toSpace, "@")
keywords_doc <- tm_map(keywords_doc, toSpace, "\\|")
keywords_doc <- tm_map(keywords_doc, toNothing, "-")
keywords_doc <- tm_map(keywords_doc, toNothing, "—")
keywords_doc <- tm_map(keywords_doc, toNothing, "–")
keywords_doc <- tm_map(keywords_doc, removeNumbers)
keywords_doc <- tm_map(keywords_doc, stripWhitespace)
keywords_doc <- tm_map(keywords_doc, removePunctuation)
keywords_doc <- tm_map(keywords_doc, removeWords, stopwords("english"))
keywords_doc <- tm_map(keywords_doc, removeWords, c("the", "can",'did','like', 'one', 'and', 'use', 'NA'))
dtm <- DocumentTermMatrix(keywords_doc)
dtm <- removeSparseTerms(dtm, 0.96)
top_terms=findMostFreqTerms(dtm, n = 20L)
top_terms=as.data.frame(do.call(rbind, top_terms))
text_stats$top_terms[k]=paste(colnames(top_terms)[1:15],sep="|-|", collapse=", ")
}
## Generate Author's Custom Categories ##
unique(text_stats$top_terms[1:50])
df1=cbind(df1_1,df1_2)
#####################################################
#### Initialize Cleanup of Google Analytics Data ####
#####################################################
df1$obs_day=as.Date(df1$date)
df1$date<-NULL
#  Remove Popular Proxy Strings
df1 <- df1[!grepl("search/srpcache", df1$pagePath),]
df1 <- df1[!grepl("www.filterbypass.me", df1$pagePath),]
df1 <- df1[!grepl("wikipedia.org/secure", df1$pagePath),]
df1 <- df1[!grepl("www.googleadservices.com", df1$pagePath),]
df1 <- df1[!grepl("bit.ly", df1$pagePath),]
df1 <- df1[!grepl("j.mp", df1$pagePath),]
df1 <- df1[!grepl("nl.hideproxy.me", df1$pagePath),]
df1 <- df1[!grepl("cc.bingj.com", df1$pagePath),]
df1 <- df1[!grepl("prolegis/getfile", df1$pagePath),]
df1 <- df1[!grepl("cluster23-files.instructure", df1$pagePath),]
df1 <- df1[!grepl("rorr.im/reddit.com", df1$pagePath),]
df1 <- df1[!grepl("www.duplichecker.com", df1$pagePath),]
df1 <- df1[!grepl("copyscape", df1$pagePath),]
df1 <- df1[!grepl("us1.proxysite", df1$pagePath),]
df1 <- df1[!grepl("eveil.alize", df1$pagePath),]
df1 <- df1[!grepl("honyaku.yahoofs.jp", df1$pagePath),]
df1 <- df1[!grepl("ow.ly", df1$pagePath),]
df1 <- df1[!grepl("searchenginereports.net", df1$pagePath),]
df1 <- df1[!grepl("xitenow", df1$pagePath),]
df1 <- df1[!grepl("cato.us1.list-manage.com/track/click", df1$pagePath),]
# More Reshaping of Page Paths & Remove Trailing Strings #
df1$pagePath2= gsub(".*www.cato.org", "www.cato.org", df1$pagePath)
df1$pagePath2= gsub(pattern="[?].*","",x=df1$pagePath2)
df1$pagePath2= gsub(pattern=".*https://*|&.*","",x=df1$pagePath2)
df1$pagePath2= gsub(pattern=".*genius.it/*|&.*",replacement="",x=df1$pagePath2)
df1$pagePath2= gsub(".*www-cato-org","www.cato.org",df1$pagePath2,perl=TRUE)
df1$pagePath2= gsub(".*www.cato.org.*?/publications","www.cato.org/publications",df1$pagePath2,perl=TRUE)
df1$pagePath2= gsub("\\.myaccess.library.utoronto.ca", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("proxy.earlham.edu", "www.cato.org", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("object.cato.org", "www.cato.org", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("seekingalpha.com", "www.cato.org", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".ezproxy.wallawalla.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".stfi.re", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".helin.uri.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".proxy.lib.pdx.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub("www.cato.org:80", "www.cato.org", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("www.cato-at-liberty.org", "www.cato.org/blog", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("www.cato/", "www.cato.org/", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("index.html", "", df1$pagePath2, perl=TRUE)
df1$pagePath2= gsub(".jpllnet.sfsu.edu", "", df1$pagePath2, perl=TRUE)
df1$pagePath2=gsub("what-have-the-politicians-in-washington-given-us/","what-have-politicians-washington-given-us",df1$pagePath2)
df1$pagePath2= gsub(".ezproxy.csusm.edu", "", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2= gsub("proxy.unfake.us/proxy/350/", "www.cato.org/blog/", df1$pagePath2, perl=TRUE) # Index.html
df1$pagePath2=ifelse(grepl("php$", df1$pagePath2)==T, df1$pagePath, df1$pagePath2)
refcols <- c("obs_day", 'pagePath', 'pagePath2')
df1 <- df1[, c(refcols, setdiff(names(df1), refcols))]
df1$pagePath1=df1$pagePath
df1$pagePath=df1$pagePath2
df1$pagePath2=NULL
#####################################################
################ Scrape Cato Web Data ###############
#####################################################
df_final= data.frame()
web_df= data.frame()
df1$ID <- seq.int(nrow(df1))
url_vector_full=df1[["pagePath"]]
url_vector=unique(url_vector_full)
SafeGet = function (x)	{
tryCatch({
#	short_url_vector
html=GET(x)
parsed=htmlParse(html)
root=xmlRoot(parsed)
title = xpathSApply(root, "//h1[@class='page-h1'][1]", xmlValue)
return(title)},
error=function(e){cat("ERROR :", conditionMessage(e))}, '0')}
website_responses <- pbmclapply(url_vector, SafeGet, mc.preschedule=T)
View(cato_scholars)
# Twitter Scraping
library(rjson)
library(twitteR)
library(ROAuth)
library(httr)
library(XML)
library(anytime)
library(syuzhet)
ClosestMatch2 =  function(string, stringVector){
stringVector[amatch(string, stringVector, maxDist=Inf)]}
#library(reticulate)
#os <- import("os")
# Set API Keys
access_token   =   '3705111012-0ZSGhm0Y5XDkptTYfecD8TwXoJTepfQ6fgtkUX2'
access_token_secret  =  'i3EaK25UsGsHvnhJzvyLxTnVOAMusH5giu0oOKf3Y0pJY'
consumer_key = 'kI3TDGtYDpdN5mPWVtZg4E74L'
consumer_secret  = 'Lk9upIVEm5BsiQq1o3KalWDWLxHL2hFnRlzwDJAkIGnSUvkr6Y'
setup_twitter_oauth(consumer_key, consumer_secret,access_token, access_token_secret)
cato_feeds= 'cato-twitter-feeds'
twlist= "cato-policy-scholars"
twowner= "CatoInstitute"
api.url= paste0("https://api.twitter.com/1.1/lists/members.json?slug=",twlist,"&owner_screen_name=",twowner,"&count=5000")
response <- GET(api.url, config(token=twitteR:::get_oauth_sig()))
load( file = "NAME_SAMPLE.RData")
#cato-twitter-feeds
response.list <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
users.names <- sapply(response.list$users, function(i) i$name)
users.screennames <- sapply(response.list$users, function(i) i$screen_name)
users.IDs <- sapply(response.list$users, function(i) i$id_str)
faves <- sapply(response.list$users, function(i) i$favourites_count)
followers <- sapply(response.list$users, function(i) i$followers_count)
date_created <- sapply(response.list$users, function(i) i$created_at)
cato_twitter=cbind(users.names,date_created,users.screennames,users.IDs,faves,followers)
scholars=as.data.frame(cato_twitter)
names(scholars)[names(scholars) == 'users.names'] ='name.twitter'
names(scholars)[names(scholars) == 'users.IDs'] ='ID.twitter'
names(scholars)[names(scholars) == 'users.screennames'] ='handle.twitter'
website.names = list()
for(i in seq_along(scholars$name.twitter)){
temp=scholars$name.twitter[i]
website.names[i] = ClosestMatch2(temp, name_sample$author_full) }
scholars$name.website=website.names
scholars=as.data.frame(scholars)
i <- sapply(scholars, is.factor)
scholars[i] <- lapply(scholars[i], as.character)
scholars$name.website=unlist(scholars$name.website)
scholars$title_category_1 = NA
scholars$category_1 = NA
scholars$title_category_2 = NA
scholars$category_2 = NA
scholarstitle_category_3 = NA
scholars$category_3 = NA
scholars$title_category_4 = NA
scholars$category_4 = NA
scholars$title_category_5 = NA
scholars$category_5 = NA
str(scholars)
write.csv(scholars, "Cato_Scholars.csv")
# Twitter Scraping
library(rjson)
library(twitteR)
library(ROAuth)
library(httr)
library(XML)
library(anytime)
library(syuzhet)
ClosestMatch2 =  function(string, stringVector){
stringVector[amatch(string, stringVector, maxDist=Inf)]}
#library(reticulate)
#os <- import("os")
# Set API Keys
access_token   =   '3705111012-0ZSGhm0Y5XDkptTYfecD8TwXoJTepfQ6fgtkUX2'
access_token_secret  =  'i3EaK25UsGsHvnhJzvyLxTnVOAMusH5giu0oOKf3Y0pJY'
consumer_key = 'kI3TDGtYDpdN5mPWVtZg4E74L'
consumer_secret  = 'Lk9upIVEm5BsiQq1o3KalWDWLxHL2hFnRlzwDJAkIGnSUvkr6Y'
setup_twitter_oauth(consumer_key, consumer_secret,access_token, access_token_secret)
cato_feeds= 'cato-twitter-feeds'
twlist= "cato-policy-scholars"
twowner= "CatoInstitute"
api.url= paste0("https://api.twitter.com/1.1/lists/members.json?slug=",twlist,"&owner_screen_name=",twowner,"&count=5000")
response <- GET(api.url, config(token=twitteR:::get_oauth_sig()))
load( file = "NAME_SAMPLE.RData")
#cato-twitter-feeds
response.list <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
users.names <- sapply(response.list$users, function(i) i$name)
users.screennames <- sapply(response.list$users, function(i) i$screen_name)
users.IDs <- sapply(response.list$users, function(i) i$id_str)
faves <- sapply(response.list$users, function(i) i$favourites_count)
followers <- sapply(response.list$users, function(i) i$followers_count)
date_created <- sapply(response.list$users, function(i) i$created_at)
cato_twitter=cbind(users.names,date_created,users.screennames,users.IDs,faves,followers)
scholars=as.data.frame(cato_twitter)
names(scholars)[names(scholars) == 'users.names'] ='name.twitter'
names(scholars)[names(scholars) == 'users.IDs'] ='ID.twitter'
names(scholars)[names(scholars) == 'users.screennames'] ='handle.twitter'
website.names = list()
for(i in seq_along(scholars$name.twitter)){
temp=scholars$name.twitter[i]
website.names[i] = ClosestMatch2(temp, name_sample$author_full) }
scholars$name.website=website.names
scholars=as.data.frame(scholars)
i <- sapply(scholars, is.factor)
scholars[i] <- lapply(scholars[i], as.character)
scholars$name.website=unlist(scholars$name.website)
scholars$title_category_1 = NA
scholars$category_1 = NA
scholars$title_category_2 = NA
scholars$category_2 = NA
scholarstitle_category_3 = NA
scholars$category_3 = NA
scholars$title_category_4 = NA
scholars$category_4 = NA
scholars$title_category_5 = NA
scholars$category_5 = NA
str(scholars)
write.csv(scholars, "Cato_Scholars.csv")
response.list <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
response.list <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
users.names <- sapply(response.list$users, function(i) i$name)
users.screennames <- sapply(response.list$users, function(i) i$screen_name)
users.IDs <- sapply(response.list$users, function(i) i$id_str)
faves <- sapply(response.list$users, function(i) i$favourites_count)
followers <- sapply(response.list$users, function(i) i$followers_count)
date_created <- sapply(response.list$users, function(i) i$created_at)
cato_twitter=cbind(users.names,date_created,users.screennames,users.IDs,faves,followers)
scholars=as.data.frame(cato_twitter)
load( file = "NAME_SAMPLE.RData")
View(name_sample)
cato_feeds= 'cato-twitter-feeds'
twlist= "cato-policy-scholars"
twowner= "CatoInstitute"
api.url= paste0("https://api.twitter.com/1.1/lists/members.json?slug=",twlist,"&owner_screen_name=",twowner,"&count=5000")
response <- GET(api.url, config(token=twitteR:::get_oauth_sig()))
View(response)
response.list <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
