sum(is.na(sweep_insert$lon))
sweep_insert
sweep
sweep$lat = -14.27806
sweep$lon = -170.7025
sweep$lat = -14.27806
sweep$lon = -170.7025
sweep_insert = merge(city_coords, sweep, by.x = 'city_name', by.y = 'fill_cities', all.x = T)
sweep_insert$lon = ifelse(!is.na(sweep_insert$lon.x), sweep_insert$lon.x,
ifelse(!is.na(sweep_insert$lon.y), sweep_insert$lon.y, NA))
sweep_insert$lat = ifelse(!is.na(sweep_insert$lat.y), sweep_insert$lat.y,
ifelse(!is.na(sweep_insert$lat.x), sweep_insert$lat.x, NA))
sweep_insert$lon.x = NULL
sweep_insert$lon.y = NULL
sweep_insert$lat.x = NULL
sweep_insert$lat.y = NULL
city_coords = sweep_insert
# city_coords$lat = ifelse((city_coords$city_name == sweep$fill_cities), sweep$lat, NA )
# city_coords$lon = ifelse((city_coords$city_name == sweep$fill_cities), sweep$lon, NA  )
sum(is.na(sweep_insert$lon))
city_coordinates = city_coords
city_coordinates = city_coords
is.na(city_coordinates)
any.na(city_coordinates)
anyNA((city_coordinates))
write.csv(city_coordinates, 'city_coordinates.csv')
setwd("~/PythonStuff/Project ArmBop/Robert/Data/")
flight_dt=read_csv('Merged_Flights.csv')
setwd("~/PythonStuff/Project ArmBop/Robert/Data/")
flight_dt=read_csv('Merged_Flights.csv')
setwd("~/PythonStuff/Project ArmBop/Robert/Data/")
flight_dt=read_csv('Merged_Flights.csv')
keys=read_csv('Keys.csv')
Ccodes=read_csv('Carrier Code.csv')
coordinates=read_csv('city_coordinates.csv')
flight_dt=setDT(flight_dt)
names(flight_dt)
names(flight_dt)=tolower(names(flight_dt))
names(flight_dt)
unique(flight_dt$origin_city_name)
names(flight_dt)=tolower(names(flight_dt))
names(flight_dt)
#unique(flight_dt$origin_city_name)
flight_dt=merge(flight_dt, coordinates, by.x = 'dest_city_name', by.y = coordinates$city_name)
flight_dt=merge(flight_dt, coordinates, by.x = 'dest_city_name', by.y = 'city_name')
names(flight_dt)[names(flight_dt) == 'lat'] <- 'dest_lat'
names(flight_dt)[names(flight_dt) == 'lon'] <- 'dest_lon'
library(rsconnect)
library(leaflet)
library(shinydashboard)
library(shiny)
library(RColorBrewer)
library(scales)
library(rsconnect)
library(shiny)
rsconnect::deployApp('/Users/rorr/PythonStuff/Project-ArmBop/Robert/map_app')
detach(shiny)
rsconnect::deployApp('/Users/rorr/PythonStuff/Project-ArmBop/Robert/map_app')
library(leaflet)
library(shinydashboard)
#library(shiny)
library(RColorBrewer)
library(scales)
library(lattice)
library(dplyr)
library(leaflet.minicharts)
library(rsconnect)
rsconnect::deployApp('/Users/rorr/PythonStuff/Project-ArmBop/Robert/map_app')
shiny::runApp('PythonStuff/Project-ArmBop/Robert/map_app')
runApp('PythonStuff/Project-ArmBop/Robert/map_app')
install.packages("stringi")
library(survey)
library(reshape2)
library(plyr)
x <- readRDS( file.path( path.expand( "~" ) , "CPSASEC" , paste(2017,"cps asec.rds") ) )
codes<-read.csv('/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Immigration/CPS_Appendix-H(CountryCodes).csv')
keeps <- c("Code", "Country")
codes<- codes[ , keeps, drop = FALSE]
# Generational Immmigration Dynamics
library(survey)				# load survey package (analyzes complex design surveys)
library(reshape2)
library(plyr)
library(openxlsx)
library(Hmisc)
codes=read.csv('/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Immigration/CPS_Appendix-H(CountryCodes).csv')
refugee_admin=read.xlsx('/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Immigration/Refugees/LPRRefugees-Final.xlsx')
str(refugee_admin)
#test=cleanme(refugee_admin)
#pop_68 = peinusyr >= 4
x=readRDS( file.path( path.expand( "~" ),"CPSASEC",paste(2017,"cps asec.rds") ) )
immigrant_dads<- subset(x, a_sex == 1 & a_age > 18 & prcitshp >= 4 , select = c(h_seq , a_lineno))
immigrant_moms<- subset(x, a_sex == 2 & a_age > 18 & prcitshp >= 4 , select = c(h_seq , a_lineno))
noncitizen_dads<- subset(x, a_sex == 1 & a_age > 18 & prcitshp >= 5 , select = c(h_seq , a_lineno))
noncitizen_moms<- subset(x, a_sex == 2 & a_age > 18 & prcitshp >= 5 , select = c(h_seq , a_lineno))
kids<- subset(x, a_age < 19, select = c(h_seq , a_lineno , prcitshp, pelndad, pelnmom))
immigrant_moms$i_mom_match <- 1
immigrant_dads$i_dad_match <- 1
noncitizen_moms$mom_match <- 1
noncitizen_dads$dad_match <- 1
before_nrow <- nrow( kids )
kids <- merge( kids , noncitizen_moms , all.x = T, by.x =c( "h_seq","pelnmom"),by.y = c( "h_seq" , "a_lineno" ) )
kids <- merge( kids , noncitizen_dads , all.x = T, by.x =c( "h_seq","pelndad"),by.y = c( "h_seq" , "a_lineno" ) )
stopifnot( nrow( kids ) == before_nrow )
kids$either_parent_noncitizen <- as.numeric( kids$mom_match %in% 1 | kids$dad_match %in% 1 )
before_nrow <- nrow( x )
x <- merge( x , kids , all.x = TRUE )
x[ is.na( x$either_parent_noncitizen ) , 'either_parent_noncitizen' ] <- 0
stopifnot( nrow( x ) == before_nrow )
kids<- subset(x, a_age < 19, select = c(h_seq , a_lineno , prcitshp, pelndad, pelnmom))
before_nrow <- nrow( kids )
kids <- merge( kids , immigrant_moms , all.x = T, by.x =c( "h_seq","pelnmom"),by.y = c( "h_seq" , "a_lineno" ) )
kids <- merge( kids , immigrant_dads , all.x = T, by.x =c( "h_seq","pelndad"),by.y = c( "h_seq" , "a_lineno" ) )
stopifnot( nrow( kids ) == before_nrow )
kids$either_parent_immigrant <- as.numeric( kids$i_mom_match %in% 1 | kids$i_dad_match %in% 1 )
before_nrow <- nrow( x )
x <- merge( x , kids , all.x = TRUE )
x[ is.na( x$either_parent_immigrant ) , 'either_parent_immigrant' ] <- 0
stopifnot( nrow( x ) == before_nrow )
table( x$either_parent_immigrant )
z <-svrepdesign(weights = ~marsupwt,
repweights = "pwwgt[1-9]",
type = "Fay",
rho = (1-1/sqrt(4)),
data = x ,
combined.weights = T)
View(z)
View(z)
survey <-svrepdesign(weights = ~marsupwt,
repweights = "pwwgt[1-9]",
type = "Fay",
rho = (1-1/sqrt(4)),
data = x ,
combined.weights = T)
ASEC_2017 <-svrepdesign(weights = ~marsupwt,
repweights = "pwwgt[1-9]",
type = "Fay",
rho = (1-1/sqrt(4)),
data = x ,
combined.weights = T)
View(ASEC_2017)
install.packages(c("eurostat", "future", "Rcpp"))
setwd("~/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)")
# Fonts
library(extrafont)
font_import()
getwd()
# My Packages
library(googleAnalyticsR)
library(tidyverse)
library(httr)
library(RCurl)
library(XML)
library(foreach)
library(stringr)
library(ggplot2)
library(data.table)
library(stringdist)
library(pbmcapply)
library(openxlsx)
#library(plyr)
save(link_df, file = "sitemap.RData")
load( file = "Big_Title_Vector.RData")
load( file = "Big_LinkedTitle.RData")
View(linked_title)
load( file = "df_intermediate.RData")
View(df_intermediate)
load(file = "sitemap.RData")
load(file = "Big_Cleaned_DAT.RData")
View(df1)
View(df_intermediate)
load( file = "Big_LinkedTitle.RData")
View(df_intermediate)
df_intermediate[!is.na(df_intermediate$title),]
df_intermediate=df_intermediate[!is.na(df_intermediate$title),]
df_intermediate=df_intermediate[(df_intermediate$title)!=NA,]
df_intermediate=df_intermediate[(df_intermediate$title)!='NA',]
View(df_intermediate)
load( file = "df_intermediate.RData")
df_intermediate=df_intermediate[(df_intermediate$title)!='NA',]
View(df_intermediate)
load(file = "responses_1.RData")
load(file = "responses_2.RData")
load(file = "responses_3.RData")
load(file = "responses_4.RData")
load(file = "responses_5.RData")
load(file = "responses_6.RData")
load(file = "responses_7.RData")
load(file = "responses_8.RData")
load(file = "responses_9.RData")
load(file = "responses_10.RData")
load(file = "responses_11.RData")
load(file = "responses_12.RData")
load(file = "responses_13.RData")
load(file = "responses_14.RData")
load(file = "responses_15.RData")
#################### Combine ##########################
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_df=as.data.frame(cbind(title=title, pagePath=url_vector))
test <-  link_df[complete.cases(link_df$title), ]
test<-link_df[-which((link_df$title == "NA")),]
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
#df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
#ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
ClosestMatch3 = function(string){url_list[amatch(string, url_list, maxDist=40,nomatch=0)]}
#ClosestMatch2(url_vector_dfi, url_list)
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
is.na(alt_page) = lengths(alt_page) == 0
alt_page[lengths(alt_page) == 0] = 0
alt_page_ <- as.data.frame(alt_page)
alt_page_ <- rbindlist(alt_page_, use.names = T, fill = T)
match_output=as.data.frame(cbind(url_vector_dfi,((alt_page))))
match_output$V2=lapply(match_output$V2,unlist)
match_output$V2=ifelse(match_output$V2=='https://www.cato.org/cato40',NA,match_output$V2)
SafeGet = function (x)	{
tryCatch({
#	short_url_vector
html=GET(x)
parsed=htmlParse(html)
root=xmlRoot(parsed)
title = xpathSApply(root, "//h1[@class='page-h1'][1]", xmlValue)
return(title)
Sys.sleep(.25)},
error=function(e){cat("ERROR :", conditionMessage(e))}, '0')}
library(stringdist)
require(XML)
library(data.table)
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_df=as.data.frame(cbind(title=title, pagePath=url_vector))
test <-  link_df[complete.cases(link_df$title), ]
test<-link_df[-which((link_df$title == "NA")),]
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
#df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
#ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
ClosestMatch3 = function(string){url_list[amatch(string, url_list, maxDist=40,nomatch=0)]}
#ClosestMatch2(url_vector_dfi, url_list)
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
is.na(alt_page) = lengths(alt_page) == 0
alt_page[lengths(alt_page) == 0] = 0
alt_page_ <- as.data.frame(alt_page)
alt_page_ <- rbindlist(alt_page_, use.names = T, fill = T)
match_output=as.data.frame(cbind(url_vector_dfi,((alt_page))))
match_output$V2=lapply(match_output$V2,unlist)
match_output$V2=ifelse(match_output$V2=='https://www.cato.org/cato40',NA,match_output$V2)
load(file = "sitemap.RData")
df_intermediate=df_intermediate[(df_intermediate$title)!='NA',]
load( file = "df_intermediate.RData")
df_intermediate=df_intermediate[(df_intermediate$title)!='NA',]
load( file = "Big_Title_Vector.RData")
load( file = "Big_LinkedTitle.RData")
View(linked_title)
library(stringdist)
require(XML)
library(data.table)
url_1=download_xml('https://www.cato.org/sitemap.xml?page=1')
url_2=download_xml('https://www.cato.org/sitemap.xml?page=2')
# Read XML 1
xmlfile <- xmlParse(url_1)
# Convert to List
tagsList <- xmlToList(xmlfile)
# Each List element is a character vector.  Convert each of these into a data.table
tagsList <- lapply(tagsList, function(x) as.data.table(as.list(x)))
# Rbind all the 1-row data.tables into a single data.table
tags_1 <- rbindlist(tagsList, use.names = T, fill = T)
tags_1=as.data.frame(tags_1)
# Read XML 2
xmlfile <- xmlParse(url_2)
# Convert to List
tagsList <- xmlToList(xmlfile)
# Each List element is a character vector.  Convert each of these into a data.table
tagsList <- lapply(tagsList, function(x) as.data.table(as.list(x)))
# Rbind all the 1-row data.tables into a single data.table
tags_2 <- rbindlist(tagsList, use.names = T, fill = T)
tags_2=as.data.frame(tags_2)
url_list=as.list(rbind(tags_1$loc, tags_2$loc))
url_vector=as.vector(url_list)
library(XML)
url_1=download_xml('https://www.cato.org/sitemap.xml?page=1')
library(xml2)
library(stringdist)
library(xml2)
library(XML)
library(data.table)
url_1=download_xml('https://www.cato.org/sitemap.xml?page=1')
url_2=download_xml('https://www.cato.org/sitemap.xml?page=2')
# Read XML 1
xmlfile <- xmlParse(url_1)
# Convert to List
tagsList <- xmlToList(xmlfile)
# Each List element is a character vector.  Convert each of these into a data.table
tagsList <- lapply(tagsList, function(x) as.data.table(as.list(x)))
# Rbind all the 1-row data.tables into a single data.table
tags_1 <- rbindlist(tagsList, use.names = T, fill = T)
tags_1=as.data.frame(tags_1)
# Read XML 2
xmlfile <- xmlParse(url_2)
# Convert to List
tagsList <- xmlToList(xmlfile)
# Each List element is a character vector.  Convert each of these into a data.table
tagsList <- lapply(tagsList, function(x) as.data.table(as.list(x)))
# Rbind all the 1-row data.tables into a single data.table
tags_2 <- rbindlist(tagsList, use.names = T, fill = T)
tags_2=as.data.frame(tags_2)
url_list=as.list(rbind(tags_1$loc, tags_2$loc))
url_vector=as.vector(url_list)
ClosestMatch3 = function(string){url_list[amatch(string, url_list, maxDist=40,nomatch=0)]}
title_vector_dfi=df_intermediate[["title"]]
View(url_list)
load( file = "Big_Title_Vector.RData")
load( file = "Big_LinkedTitle.RData")
load( file = "Big_Title_Vector.RData")
load( file = "Big_LinkedTitle.RData")
View(linked_title)
load(file = "responses_1.RData")
load(file = "responses_2.RData")
load(file = "responses_3.RData")
load(file = "responses_4.RData")
load(file = "responses_5.RData")
load(file = "responses_6.RData")
load(file = "responses_7.RData")
load(file = "responses_8.RData")
load(file = "responses_9.RData")
load(file = "responses_10.RData")
load(file = "responses_11.RData")
load(file = "responses_12.RData")
load(file = "responses_13.RData")
load(file = "responses_14.RData")
load(file = "responses_15.RData")
View(responses_1)
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_title=as.data.frame(cbind(title=title, pagePath=url_vector))
SafeGet = function (x)	{
tryCatch({
#	short_url_vector
html=GET(x)
parsed=htmlParse(html)
root=xmlRoot(parsed)
title = xpathSApply(root, "//h1[@class='page-h1'][1]", xmlValue)
return(title)
Sys.sleep(.25)},
error=function(e){cat("ERROR :", conditionMessage(e))}, '0')}
library(stringdist)
library(xml2)
library(XML)
library(data.table)
url_1=download_xml('https://www.cato.org/sitemap.xml?page=1')
url_2=download_xml('https://www.cato.org/sitemap.xml?page=2')
# Read XML 1
xmlfile <- xmlParse(url_1)
# Convert to List
tagsList <- xmlToList(xmlfile)
# Each List element is a character vector.  Convert each of these into a data.table
tagsList <- lapply(tagsList, function(x) as.data.table(as.list(x)))
# Rbind all the 1-row data.tables into a single data.table
tags_1 <- rbindlist(tagsList, use.names = T, fill = T)
tags_1=as.data.frame(tags_1)
# Read XML 2
xmlfile <- xmlParse(url_2)
# Convert to List
tagsList <- xmlToList(xmlfile)
# Each List element is a character vector.  Convert each of these into a data.table
tagsList <- lapply(tagsList, function(x) as.data.table(as.list(x)))
# Rbind all the 1-row data.tables into a single data.table
tags_2 <- rbindlist(tagsList, use.names = T, fill = T)
tags_2=as.data.frame(tags_2)
url_list=as.list(rbind(tags_1$loc, tags_2$loc))
url_vector=as.vector(url_list)
load(file = "responses_1.RData")
load(file = "responses_2.RData")
load(file = "responses_3.RData")
load(file = "responses_4.RData")
load(file = "responses_5.RData")
load(file = "responses_6.RData")
load(file = "responses_7.RData")
load(file = "responses_8.RData")
load(file = "responses_9.RData")
load(file = "responses_10.RData")
load(file = "responses_11.RData")
load(file = "responses_12.RData")
load(file = "responses_13.RData")
load(file = "responses_14.RData")
load(file = "responses_15.RData")
#################### Combine ##########################
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_title=as.data.frame(cbind(title=title, pagePath=url_vector))
test <-  link_title[complete.cases(link_title$title), ]
test<-link_title[-which((link_title$title == "NA")),]
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
#ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
ClosestMatch3 = function(string){url_list[amatch(string, url_list, maxDist=40,nomatch=0)]}
#ClosestMatch2(url_vector_dfi, url_list)
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
is.na(alt_page) = lengths(alt_page) == 0
alt_page[lengths(alt_page) == 0] = 0
alt_page_ <- as.data.frame(alt_page)
alt_page_ <- rbindlist(alt_page_, use.names = T, fill = T)
match_output=as.data.frame(cbind(url_vector_dfi,((alt_page))))
match_output$V2=lapply(match_output$V2,unlist)
match_output$V2=ifelse(match_output$V2=='https://www.cato.org/cato40',NA,match_output$V2)
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
#df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
#ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
ClosestMatch3 = function(string){url_list[amatch(string, url_list, maxDist=40,nomatch=0)]}
#ClosestMatch2(url_vector_dfi, url_list)
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
is.na(alt_page) = lengths(alt_page) == 0
alt_page[lengths(alt_page) == 0] = 0
alt_page_ <- as.data.frame(alt_page)
alt_page_ <- rbindlist(alt_page_, use.names = T, fill = T)
match_output=as.data.frame(cbind(url_vector_dfi,((alt_page))))
match_output$V2=lapply(match_output$V2,unlist)
match_output$V2=ifelse(match_output$V2=='https://www.cato.org/cato40',NA,match_output$V2)
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
load( file = "df_intermediate.RData")
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_title=as.data.frame(cbind(title=title, pagePath=url_vector))
test <-  link_title[complete.cases(link_title$title), ]
test<-link_title[-which((link_title$title == "NA")),]
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
#df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
load( file = "df_intermediate.RData")
df_intermediate=df_intermediate[(df_intermediate$title)!='NA',]
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_title=as.data.frame(cbind(title=title, pagePath=url_vector))
test <-  link_title[complete.cases(link_title$title), ]
test<-link_title[-which((link_title$title == "NA")),]
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
#df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
#ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
ClosestMatch3 = function(string){url_list[amatch(string, url_list, maxDist=40,nomatch=0)]}
#ClosestMatch2(url_vector_dfi, url_list)
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
is.na(alt_page) = lengths(alt_page) == 0
alt_page[lengths(alt_page) == 0] = 0
alt_page_ <- as.data.frame(alt_page)
alt_page_ <- rbindlist(alt_page_, use.names = T, fill = T)
match_output=as.data.frame(cbind(url_vector_dfi,((alt_page))))
match_output$V2=lapply(match_output$V2,unlist)
match_output$V2=ifelse(match_output$V2=='https://www.cato.org/cato40',NA,match_output$V2)
ClosestMatch3 = function(string){url_vector[amatch(string, url_vector, maxDist=40,nomatch=0)]}
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
url_vector_dfi
url_vector_dfi=df_int_fail[["pagePath"]]
df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
#df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_fail[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
url_vector_dfi=url_vector_dfi[nchar(url_vector_dfi) > 16]
ClosestMatch3 = function(string){stringVector[amatch(string, stringVector, maxDist=40,nomatch=0)]}
load( file = "df_intermediate.RData")
df_intermediate=df_intermediate[(df_intermediate$title)!='NA',]
website_responses=(c(responses_1,responses_2,responses_3,responses_4,responses_5,responses_6,responses_7,
responses_8,responses_9,responses_10,responses_11, responses_12,responses_13,responses_14,responses_15))
is.na(website_responses) = lengths(website_responses) == 0
website_responses[lengths(website_responses) == 0] = NA
title=trimws(website_responses)
link_title=as.data.frame(cbind(title=title, pagePath=url_vector))
test <-  link_title[complete.cases(link_title$title), ]
test<-link_title[-which((link_title$title == "NA")),]
#df_int_fail <- subset(df_intermediate, (df_intermediate$title)=="NA")
df_int_success <- subset(df_intermediate, (df_intermediate$title)!="NA")
url_vector_dfi=df_int_success[["pagePath"]]
url_vector_dfi=as.vector(unique(url_vector_dfi))
ClosestMatch3 = function(string){url_vector[amatch(string, url_vector, maxDist=40,nomatch=0)]}
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
#ClosestMatch2 = function(string, stringVector){stringVector[amatch(string, stringVector, maxDist=Inf,nomatch=0)]}
ClosestMatch3 = function(string){url_vector[amatch(string, url_vector, maxDist=40,nomatch=0)]}
#ClosestMatch2(url_vector_dfi, url_list)
alt_page=pbmclapply(url_vector_dfi, ClosestMatch3)
View(link_title)
