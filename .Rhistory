emp_growth= merge( tr, t , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( emp_growth, tl , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( emp_growth, g , by=c('occup_name','occup_major','det_occupation_name'),all = T)
names(emp_growth)
emp_growth=subset(emp_growth,occup_name!='Not in universe or children'&occup_major!='Not in universe or children'&det_occupation_name!='Not in universe or children')
emp_growth$license_rate_avg <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_rate_Δ <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_req_rate_avg <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$license_req_rate_Δ <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$gender_avg <- with(emp_growth, ((gender_18/gender_17)-1)*100)
emp_growth$occ_rate_Δ <- with(emp_growth, ((count_18/count_17)-1)*100)
emp_growth$occ_number_Δ <- with(emp_growth, ((count_18-count_17)))
write.csv(emp_growth, "occupational_growth.csv")
tr17=svyby(~license3, by=~occup_name+occup_major+det_occupation_name, design=z2017_work_age,svymean, na.rm=T)
tr18=svyby(~license3, by=~occup_name+occup_major+det_occupation_name, design=z2018_work_age,svymean, na.rm=T)
tl17=svyby(~license2, by=~occup_name+occup_major+det_occupation_name, design=z2017_work_age,svymean, na.rm=T)
tl18=svyby(~license2, by=~occup_name+occup_major+det_occupation_name, design=z2018_work_age,svymean, na.rm=T)
t17=svyby(~one, by=~occup_name+occup_major+det_occupation_name, design=z2017_work_age,svytotal, na.rm=T)
t18=svyby(~one, by=~occup_name+occup_major+det_occupation_name, design=z2018_work_age,svytotal, na.rm=T)
g17=svyby(~male, by=~occup_name+occup_major+det_occupation_name, design=z2017_work_age,svymean, na.rm=T)
g18=svyby(~male, by=~occup_name+occup_major+det_occupation_name, design=z2018_work_age,svymean, na.rm=T)
colnames(tl17)[colnames(tl17)=="license2"]="license_rate_17"
colnames(tl18)[colnames(tl18)=="license2"]="license_rate_18"
colnames(tr17)[colnames(tr17)=="license3"]="license_req_rate_17"
colnames(tr18)[colnames(tr18)=="license3"]="license_req_rate_18"
colnames(tr17)[colnames(tr17)=="se"]="se_rate_17"
colnames(tr18)[colnames(tr18)=="se"]="se_rate_18"
colnames(tl17)[colnames(tl17)=="se"]="se_req_rate_17"
colnames(tl18)[colnames(tl18)=="se"]="se_req_rate_18"
colnames(t17)[colnames(t17)=="one"]="count_17"
colnames(t18)[colnames(t18)=="one"]="count_18"
colnames(t17)[colnames(t17)=="se"]="se_17"
colnames(t18)[colnames(t18)=="se"]="se_18"
colnames(g17)[colnames(g17)=="male"]="gender_17"
colnames(g18)[colnames(g18)=="male"]="gender_18"
colnames(g17)[colnames(g17)=="se"]="se_gender_17"
colnames(g18)[colnames(g18)=="se"]="se_gender_18"
tl=merge(tl17,tl18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
tr=merge(tr17,tr18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
t= merge( t17, t18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
g= merge( g17, g18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( tr, t , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( emp_growth, tl , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( emp_growth, g , by=c('occup_name','occup_major','det_occupation_name'),all = T)
names(emp_growth)
emp_growth=subset(emp_growth,occup_name!='Not in universe or children'&occup_major!='Not in universe or children'&det_occupation_name!='Not in universe or children')
emp_growth$license_rate_avg <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_rate_Δ <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_req_rate_avg <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$license_req_rate_Δ <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$gender_avg <- with(emp_growth, ((gender_18/gender_17)-1)*100)
emp_growth$occ_rate_Δ <- with(emp_growth, ((count_18/count_17)-1)*100)
emp_growth$occ_number_Δ <- with(emp_growth, ((count_18-count_17)))
write.csv(emp_growth, "occupational_growth.csv")
svymean(~as.factor(male),z2018_work_age)
emp_growth$gender_avg <- with(emp_growth, ((gender_18/gender_17))*100)
emp_growth$occ_rate_Δ <- with(emp_growth, ((count_18/count_17)-1)*100)
emp_growth$occ_number_Δ <- with(emp_growth, ((count_18-count_17)))
write.csv(emp_growth, "occupational_growth.csv")
View(emp_growth)
z2017_labor=subset(z2017,prpertyp==2&a_age%in%25:64&pemlr%in%1:4)
z2018_labor=subset(z2018,prpertyp==2&a_age%in%25:64&pemlr%in%1:4)
tr17=svyby(~license3, by=~occup_name+occup_major+det_occupation_name, design=z2017_labor,svymean, na.rm=T)
tr18=svyby(~license3, by=~occup_name+occup_major+det_occupation_name, design=z2018_labor,svymean, na.rm=T)
tl17=svyby(~license2, by=~occup_name+occup_major+det_occupation_name, design=z2017_labor,svymean, na.rm=T)
tl18=svyby(~license2, by=~occup_name+occup_major+det_occupation_name, design=z2018_labor,svymean, na.rm=T)
t17=svyby(~one, by=~occup_name+occup_major+det_occupation_name, design=z2017_labor,svytotal, na.rm=T)
t18=svyby(~one, by=~occup_name+occup_major+det_occupation_name, design=z2018_labor,svytotal, na.rm=T)
g17=svyby(~male, by=~occup_name+occup_major+det_occupation_name, design=z2017_labor,svymean, na.rm=T)
g18=svyby(~male, by=~occup_name+occup_major+det_occupation_name, design=z2018_labor,svymean, na.rm=T)
colnames(tl17)[colnames(tl17)=="license2"]="license_rate_17"
colnames(tl18)[colnames(tl18)=="license2"]="license_rate_18"
colnames(tr17)[colnames(tr17)=="license3"]="license_req_rate_17"
colnames(tr18)[colnames(tr18)=="license3"]="license_req_rate_18"
colnames(tr17)[colnames(tr17)=="se"]="se_rate_17"
colnames(tr18)[colnames(tr18)=="se"]="se_rate_18"
colnames(tl17)[colnames(tl17)=="se"]="se_req_rate_17"
colnames(tl18)[colnames(tl18)=="se"]="se_req_rate_18"
colnames(t17)[colnames(t17)=="one"]="count_17"
colnames(t18)[colnames(t18)=="one"]="count_18"
colnames(t17)[colnames(t17)=="se"]="se_17"
colnames(t18)[colnames(t18)=="se"]="se_18"
colnames(g17)[colnames(g17)=="male"]="gender_17"
colnames(g18)[colnames(g18)=="male"]="gender_18"
colnames(g17)[colnames(g17)=="se"]="se_gender_17"
colnames(g18)[colnames(g18)=="se"]="se_gender_18"
#svymean(~as.factor(male),z2018_work_age)
tl=merge(tl17,tl18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
tr=merge(tr17,tr18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
t= merge( t17, t18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
g= merge( g17, g18 , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( tr, t , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( emp_growth, tl , by=c('occup_name','occup_major','det_occupation_name'),all = T)
emp_growth= merge( emp_growth, g , by=c('occup_name','occup_major','det_occupation_name'),all = T)
names(emp_growth)
emp_growth=subset(emp_growth,occup_name!='Not in universe or children'&occup_major!='Not in universe or children'&det_occupation_name!='Not in universe or children')
emp_growth$license_rate_avg <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_rate_Δ <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_req_rate_avg <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$license_req_rate_Δ <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$gender_avg <- with(emp_growth, ((gender_18/gender_17))*100)
emp_growth$occ_rate_Δ <- with(emp_growth, ((count_18/count_17)-1)*100)
emp_growth$occ_number_Δ <- with(emp_growth, ((count_18-count_17)))
write.csv(emp_growth, "occupational_growth.csv")
emp_growth=subset(emp_growth,occup_name!='Not in universe or children'&occup_major!='Not in universe or children'&det_occupation_name!='Not in universe or children')
emp_growth$license_rate_avg <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_rate_Δ <- with(emp_growth, ((license_rate_17*license_rate_18)/2)*100)
emp_growth$license_req_rate_avg <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$license_req_rate_Δ <- with(emp_growth, ((license_req_rate_17*license_req_rate_18)/2)*100)
emp_growth$gender_avg <- with(emp_growth, ((gender_18+gender_17))/2)
emp_growth$occ_rate_Δ <- with(emp_growth, ((count_18/count_17)-1)*100)
emp_growth$occ_number_Δ <- with(emp_growth, ((count_18-count_17)))
write.csv(emp_growth, "occupational_growth.csv")
library(lodown)
# examine all available CPSBASIC microdata files
cpsbasic_cat = get_catalog( "cpsbasic" ,
output_dir = file.path( path.expand( "~" ) , "CPSBASIC" ) )
#cpsbasic_cat <- subset( cpsbasic_cat , year%in%2015:2018)
cpsbasic_cat <- subset( cpsbasic_cat , year%in%2016:2018 & month%in%1:12 )
cpsbasic_cat <- lodown( "cpsbasic" , cpsbasic_cat )
View(cpsbasic_cat)
cpsbasic_cat <- subset( cpsbasic_cat , year%in%2018 & month%in%1:12 )
cpsbasic_cat = get_catalog( "cpsbasic" ,output_dir = file.path( path.expand( "~" ) , "CPSBASIC" ) )
#cpsbasic_cat <- subset( cpsbasic_cat , year%in%2015:2018)
cpsbasic_cat <- subset( cpsbasic_cat , year%in%2018 & month%in%1:12 )
cpsbasic_cat <- lodown( "cpsbasic" , cpsbasic_cat )
sessionInfo()
sessionInfo()
# installing/loading the package:
if(!require(installr)) {
install.packages("installr");
require(installr)
} #load / install+load installr
# Google Analytics
setwd("~/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)")
library(tidyverse)
library(httr)
library(RCurl)
library(XML)
library(foreach)
library(stringr)
library(ggplot2)
library(data.table)
library(stringdist)
library(pbmcapply)
library(openxlsx)
library(tools)
#library(plyr)
###########################################################################
################## Text Analysis - Generate Text Wall #####################
###########################################################################
load(file = "df_clean_type_date_body.RData")
names(df_clean_type_date_body)
text_wall=df_clean_type_date_body %>% distinct(title, body)
text_wall=text_wall[!duplicated(text_wall),]
text_wall=text_wall[(text_wall$title)!='NA',]
text_wall=text_wall[(text_wall$body)!='NA',]
text_wall$body=as.character(text_wall$body);text_wall$title=as.character(text_wall$title)
big_text_wall <- paste(text_wall$title,text_wall$body);rm(df_clean_type_date_body)
library(tm);library(wordcloud);library(topicmodels);library(quanteda)
toSpace=content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing=content_transformer(function (x , pattern ) gsub(pattern, "", x))
big_text_wall=pbmclapply(big_text_wall, removeWords, stopwords("english"), mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, removeNumbers, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, tolower, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, removeWords, c("the", "can",'did','like', 'and', 'null', 'one', 'NA','character','list', 'immigrants','118', '399', 'much','this', 'but','also',"'s", "datetimestamp", 'will',"hour","author","content",'description','heading', "sec",'origin','min','meta'), mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, toTitleCase, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, stripWhitespace, mc.preschedule=T)
save_docs=paste(big_text_wall, sep="", collapse="")
save_docs=pbmclapply(save_docs, removePunctuation, mc.preschedule=T)
doc_text=Corpus(VectorSource(save_docs))
doc_text=tm_map(doc_text, PlainTextDocument)
doc_text=tm_map(doc_text, removePunctuation)
save(doc_text,file="doc_text.RData")
rm(df_clean_type_date_body,text_wall,toNothing,toSpace,big_text_wall,doc_text,save_docs)
doc_text
load('doc_text.RData')
doc_text
print(doc_text)
View(doc_text)
load(file = "df_clean_type_date_body.RData")
names(df_clean_type_date_body)
text_wall=df_clean_type_date_body %>% distinct(title, body)
text_wall=text_wall[!duplicated(text_wall),]
text_wall=text_wall[(text_wall$title)!='NA',]
text_wall=text_wall[(text_wall$body)!='NA',]
text_wall$body=as.character(text_wall$body);text_wall$title=as.character(text_wall$title)
big_text_wall <- paste(text_wall$title,text_wall$body);rm(df_clean_type_date_body)
library(tm);library(wordcloud);library(topicmodels);library(quanteda)
toSpace=content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing=content_transformer(function (x , pattern ) gsub(pattern, "", x))
big_text_wall=pbmclapply(big_text_wall, removeWords, stopwords("english"), mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, removeNumbers, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, tolower, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, removeWords, c("the", "can",'did','like', 'and', 'null', 'one', 'NA','character','list', 'immigrants','118', '399', 'much','this', 'but','also',"'s", "datetimestamp", 'will',"hour","author","content",'description','heading', "sec",'origin','min','meta'), mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, toTitleCase, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, stripWhitespace, mc.preschedule=T)
save_docs=paste(big_text_wall, sep="", collapse="")
save_docs=pbmclapply(save_docs, removePunctuation, mc.preschedule=T)
doc_text=Corpus(VectorSource(save_docs))
#doc_text=tm_map(doc_text, PlainTextDocument)
doc_text=tm_map(doc_text, removePunctuation)
doc_text=tm_map(doc_text, stripWhitespace)
save(doc_text,file="doc_text.RData")
rm(df_clean_type_date_body,text_wall,toNothing,toSpace,big_text_wall,doc_text,save_docs)
load(file = "df_clean_type_date_body.RData")
names(df_clean_type_date_body)
text_wall=df_clean_type_date_body %>% distinct(title, body)
text_wall=text_wall[!duplicated(text_wall),]
text_wall=text_wall[(text_wall$title)!='NA',];text_wall=text_wall[(text_wall$body)!='NA',]
text_wall$body=as.character(text_wall$body);text_wall$title=as.character(text_wall$title)
big_text_wall <- paste(text_wall$title,text_wall$body);rm(df_clean_type_date_body)
library(tm);library(wordcloud);library(topicmodels);library(quanteda)
toSpace=content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing=content_transformer(function (x , pattern ) gsub(pattern, "", x))
big_text_wall=pbmclapply(big_text_wall, removeWords, stopwords("english"), mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, removeNumbers, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, tolower, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, removeWords, c("the", "can",'did','like', 'and', 'null', 'one', 'NA','character','list', 'immigrants','118', '399', 'much','this', 'but','also',"'s", "datetimestamp", 'will',"hour","author","content",'description','heading', "sec",'origin','min','meta'), mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, toTitleCase, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, stripWhitespace, mc.preschedule=T)
big_text_wall=pbmclapply(big_text_wall, stripWhitespace, mc.preschedule=T)
save_docs=paste(big_text_wall, sep="", collapse="")
doc_text1=Corpus(VectorSource(save_docs))
doc_text1=tm_map(doc_text1, toSpace, "/")
doc_text1=tm_map(doc_text1, toSpace, "@")
doc_text1=tm_map(doc_text1, toSpace, "\\|")
doc_text1=tm_map(doc_text1, toNothing, "-")
doc_text1=tm_map(doc_text1, toNothing, "—")
doc_text1=tm_map(doc_text1, toNothing, "–")
doc_text1=VCorpus(VectorSource(save_docs))
doc_text1=tm_map(doc_text1, toSpace, "/")
doc_text1=tm_map(doc_text1, toSpace, "@")
doc_text1=tm_map(doc_text1, toSpace, "\\|")
doc_text1=tm_map(doc_text1, toNothing, "-")
doc_text1=tm_map(doc_text1, toNothing, "—")
doc_text1=tm_map(doc_text1, toNothing, "–")
doc_text1=tm_map(doc_text1, PlainTextDocument)
doc_text1=tm_map(doc_text1, removePunctuation)
doc_text=tm_map(doc_text1, stripWhitespace)
save(doc_text,file="doc_text.RData")
# Open Data
#setwd('/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/')
load( file = '/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/df_clean_type_date_body.RData')
#noTerror.RData
load( file = "/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/doc_text.RData")
#Load Dependencies
df_final=df_clean_type_date_body
rm(df_clean_type_date_body)
names(df_final)
library(tinytex)
library(wordcloud)
library(topicmodels)
library(googleAnalyticsR)
library(dplyr)
library(httr)
library(rmarkdown)
library(extrafont)
library(RCurl)
library(XML)
library(foreach)
library(stringr)
library(ggplot2)
library(lubridate)
library(quanteda)
library(tm)
loadfonts()
#Read custom functions
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
simpleCap <- function(x) {
s <- strsplit(x, " ")[[1]]
if (!is.na(s[1])) {return(paste(toupper(substring(s, 1, 1)), substring(s, 2), sep = "", collapse = " "))} else {
return(NA)}}
my_theme <- function(){theme_light() +theme(text = element_text(family = "Open Sans"), legend.text.align = 0,
plot.title = element_text(size = 12, color = "gray30"),   # Set up the title style
plot.subtitle = element_text(size = 10, color = "black"), # Set up the subtitle style
plot.margin = unit(c(.05,.05,.05,.05), "cm"),                 # Add white space at the top and left
panel.grid = element_blank(),#panel.border = element_blank(),
axis.title = element_blank(),axis.ticks = element_blank(),#axis.text.x = element_blank(),
axis.text.y = element_text(size = 9, color = "gray10"))}
current_date=format(Sys.time(), "%Y-%m-%d")
current_date=ymd(current_date)
#name="Vanessa Brown Calder"
begin=min(df_final$obs_day)
end=max(df_final$obs_day)
name=df_final$author[1]
name=as.character(name)
name
# Build file name
from_s = (begin)
from_m = as.character(begin)
from_y=str_sub(begin, start=3, end = 4)
from_m=str_sub(begin, start=6, end = 7)
to_y=str_sub(end, start=3, end = 4)
to_m=str_sub(end, start=6, end = 7)
analysis_range=paste0("(",from_m,from_y,'-',to_m,to_y,")")
initials <- function(a, b){
a <- str_split(a, "&")
a1 <- lapply(a, function(x){
x1 <- str_split(str_trim(x), " ")
paste0(unlist(lapply(x1, str_sub, 1, 2)), collapse="")
})
paste0(unlist(a1), b)
}
analysis_identifier=initials(name,analysis_range)
# creating of document matrix
library(tm)
library(wordcloud)
library(topicmodels)
library(quanteda)
tdm <- TermDocumentMatrix(doc_text)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#head(d, 10)
set.seed(12)
wordcloud(words = d$word, freq = d$freq, min.freq = 30, max.words=100,random.order = FALSE, scale=c(2.1,.7), #res=300,
rot.per=.1,vfont=c("sans serif","plain"),colors=brewer.pal(8, "Dark2"))
# Open Data
#setwd('/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/')
load( file = '/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/df_clean_type_date_body.RData')
#noTerror.RData
load( file = "/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/doc_text.RData")
#Load Dependencies
df_final=df_clean_type_date_body
rm(df_clean_type_date_body)
names(df_final)
library(tinytex)
library(wordcloud)
library(topicmodels)
library(googleAnalyticsR)
library(dplyr)
library(httr)
library(rmarkdown)
library(extrafont)
library(RCurl)
library(XML)
library(foreach)
library(stringr)
library(ggplot2)
library(lubridate)
library(quanteda)
library(tm)
loadfonts()
#Read custom functions
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
simpleCap <- function(x) {
s <- strsplit(x, " ")[[1]]
if (!is.na(s[1])) {return(paste(toupper(substring(s, 1, 1)), substring(s, 2), sep = "", collapse = " "))} else {
return(NA)}}
my_theme <- function(){theme_light() +theme(text = element_text(family = "Open Sans"), legend.text.align = 0,
plot.title = element_text(size = 12, color = "gray30"),   # Set up the title style
plot.subtitle = element_text(size = 10, color = "black"), # Set up the subtitle style
plot.margin = unit(c(.05,.05,.05,.05), "cm"),                 # Add white space at the top and left
panel.grid = element_blank(),#panel.border = element_blank(),
axis.title = element_blank(),axis.ticks = element_blank(),#axis.text.x = element_blank(),
axis.text.y = element_text(size = 9, color = "gray10"))}
current_date=format(Sys.time(), "%Y-%m-%d")
current_date=ymd(current_date)
#name="Vanessa Brown Calder"
begin=min(df_final$obs_day)
end=max(df_final$obs_day)
name=df_final$author[1]
name=as.character(name)
name
# Build file name
from_s = (begin)
from_m = as.character(begin)
from_y=str_sub(begin, start=3, end = 4)
from_m=str_sub(begin, start=6, end = 7)
to_y=str_sub(end, start=3, end = 4)
to_m=str_sub(end, start=6, end = 7)
analysis_range=paste0("(",from_m,from_y,'-',to_m,to_y,")")
initials <- function(a, b){
a <- str_split(a, "&")
a1 <- lapply(a, function(x){
x1 <- str_split(str_trim(x), " ")
paste0(unlist(lapply(x1, str_sub, 1, 2)), collapse="")
})
paste0(unlist(a1), b)
}
analysis_identifier=initials(name,analysis_range)
# creating of document matrix
library(tm)
library(wordcloud)
library(topicmodels)
library(quanteda)
tdm <- TermDocumentMatrix(doc_text)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#head(d, 10)
set.seed(12)
wordcloud(words = d$word, freq = d$freq, min.freq = 30, max.words=100,random.order = FALSE, scale=c(2.1,.7), #res=300,
rot.per=.1,vfont=c("sans serif","plain"),colors=brewer.pal(8, "Dark2"))
total_line =subset(df_final, type != "events"&type !="multimedia") # Remove events
names(df_final)
rm(df_clean_type_date_body,text_wall,toNothing,toSpace,big_text_wall,doc_text,save_docs,doc_text1)
load(file = "df_clean_type_date_body.RData")
df_final=df_clean_type_date_body
rm(df_clean_type_date_body)
df_final$type=df_final$Type
df_final$Type=NULL
save(df_final,file="df_final.RData")
# Open Data
#setwd('/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/')
load( file = '/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/df_final.RData')
#noTerror.RData
load( file = "/Users/rorr/Desktop/Welfare_Policy/Data/Data_Explorations/Google_Analytics(Cato)/doc_text.RData")
#Load Dependencies
library(tinytex)
library(wordcloud)
library(topicmodels)
library(googleAnalyticsR)
library(dplyr)
library(httr)
library(rmarkdown)
library(extrafont)
library(RCurl)
library(XML)
library(foreach)
library(stringr)
library(ggplot2)
library(lubridate)
library(quanteda)
library(tm)
loadfonts()
#Read custom functions
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
toNothing <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
simpleCap <- function(x) {
s <- strsplit(x, " ")[[1]]
if (!is.na(s[1])) {return(paste(toupper(substring(s, 1, 1)), substring(s, 2), sep = "", collapse = " "))} else {
return(NA)}}
my_theme <- function(){theme_light() +theme(text = element_text(family = "Open Sans"), legend.text.align = 0,
plot.title = element_text(size = 12, color = "gray30"),   # Set up the title style
plot.subtitle = element_text(size = 10, color = "black"), # Set up the subtitle style
plot.margin = unit(c(.05,.05,.05,.05), "cm"),                 # Add white space at the top and left
panel.grid = element_blank(),#panel.border = element_blank(),
axis.title = element_blank(),axis.ticks = element_blank(),#axis.text.x = element_blank(),
axis.text.y = element_text(size = 9, color = "gray10"))}
current_date=format(Sys.time(), "%Y-%m-%d")
current_date=ymd(current_date)
#name="Vanessa Brown Calder"
# begin=min(df_final$obs_day)
# end=max(df_final$obs_day)
# name=df_final$author[1]
# name=as.character(name)
# Build file name
from_s = (begin)
total_line =subset(df_final, type != "events"&type !="multimedia") # Remove events
total_line=aggregate(sessions ~ title + author_categories + obs_day, total_line, sum)
total_line =subset(df_final, type != "events"&type !="multimedia") # Remove events
total_line=aggregate(sessions ~ title + obs_day, total_line, sum)
total_line$big_day = paste0(total_line$title,"(",as.character(total_line$obs_day),")")
title_big=aggregate(sessions~title+obs_day+big_day, total_line,function(x) x[which.max(abs(x))])
title_big$rank_unique = rank(-title_big$sessions)
total_line=merge(total_line,title_big, all.x=T)
title_numb=aggregate(sessions~title, total_line,function(x) x[which.max(abs(x))])
title_numb$rank_overall = rank(-title_numb$sessions)
total_line=merge(total_line,title_numb, all.x=T)
total_line$rank_unique = ifelse(total_line$rank_unique > 20, NA, total_line$rank_unique)
total_line$rank_overall = ifelse(total_line$rank_overall > 20, NA, total_line$rank_overall)
top_days=total_line %>%ggplot(aes(x=obs_day,y=sessions,group=author_categories,colour=author_categories))+geom_line()+
theme(axis.text.x=element_text(angle=90,hjust = 1),legend.position="none")+my_theme()+
theme(legend.position="bottom", legend.box = "horizontal")+theme(legend.title=element_blank())+
scale_y_continuous(expand = c(0, .3))
library(ggrepel)
top_days + geom_label_repel(data = subset(total_line %>% group_by(title) %>% top_n(n=1, sessions))
,	aes(label=rank_unique), size=8)+ theme(legend.position = "None")   # label
total_line =subset(df_final, type != "Events"&type !="Multimedia") # Remove events
total_line=aggregate(sessions ~ title + obs_day + authur, total_line, sum)
names(df_final)
total_line =subset(df_final, type != "Events"&type !="Multimedia") # Remove events
total_line=aggregate(sessions ~ title + obs_day + author_full, total_line, sum)
total_line$big_day = paste0(total_line$title,"(",as.character(total_line$obs_day),")")
title_big=aggregate(sessions~title+obs_day+big_day, total_line,function(x) x[which.max(abs(x))])
title_big$rank_unique = rank(-title_big$sessions)
total_line=merge(total_line,title_big, all.x=T)
title_numb=aggregate(sessions~title, total_line,function(x) x[which.max(abs(x))])
title_numb$rank_overall = rank(-title_numb$sessions)
total_line=merge(total_line,title_numb, all.x=T)
total_line$rank_unique = ifelse(total_line$rank_unique > 20, NA, total_line$rank_unique)
total_line$rank_overall = ifelse(total_line$rank_overall > 20, NA, total_line$rank_overall)
top_days=total_line %>%ggplot(aes(x=obs_day,y=sessions,group=author_full,colour=author_full))+geom_line()+
theme(axis.text.x=element_text(angle=90,hjust = 1),legend.position="none")+my_theme()+
theme(legend.position="bottom", legend.box = "horizontal")+theme(legend.title=element_blank())+
scale_y_continuous(expand = c(0, .3))
library(ggrepel)
top_days + geom_label_repel(data = subset(total_line %>% group_by(title) %>% top_n(n=1, sessions))
,	aes(label=rank_unique), size=8)+ theme(legend.position = "None")   # label
library(knitr)
library(kableExtra)
total_table = subset(total_line, rank_unique < 16, select=-c(sessions, big_day, rank_unique)
)
total_table %>% kable() %>%  kable_styling()
Bar_type_df =subset(df_final, type != "events" & type != 'node') #& co_authors == "Caleb Brown")
# Remove events
Bar_type_df$collaboration_yn=ifelse(Bar_type_df$author!=Bar_type_df$author_full & length(Bar_type_df$author_full)!=0, "Co-Authored", ifelse(Bar_type_df$author==Bar_type_df$author_full, "Sole Author",0))
Bar_type_df<-Bar_type_df %>% distinct(type,title,collaboration_yn,one)
ggplot(data=Bar_type_df,aes(x=reorder(type,type, function(x)length(x)),y=one,fill=collaboration_yn))+
ggtitle(sprintf("Total Content: %s",sum(Bar_type_df$one)),
subtitle=sprintf("",sum(Bar_type_df$one)))+coord_flip()+
geom_bar(stat="identity", width=0.8)+ my_theme()+theme(legend.position="bottom",
legend.box = "horizontal")+theme(legend.title=element_blank())+
scale_y_continuous(expand = c(0, .3))
load(file = "df_clean_type_date_body.RData")
names(df_clean_type_date_body)
df_final=df_clean_type_date_body
rm(df_clean_type_date_body)
